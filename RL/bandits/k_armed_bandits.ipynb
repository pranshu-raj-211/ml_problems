{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of actions\n",
    "k = 10\n",
    "# number of trials\n",
    "iterations = 1000\n",
    "# values for creating the true mean distribution\n",
    "true_mean_dist_mean = 0.0\n",
    "true_mean_dist_stddev = 1.0\n",
    "# defining optimal mean values for stationary reward probability distribution\n",
    "true_means = np.random.normal(loc=true_mean_dist_mean, scale = true_mean_dist_stddev, size=k)\n",
    "# initial estimate of mean of value of actions\n",
    "estimated_means = np.zeros(k)\n",
    "\n",
    "optimal_action = np.argmax(true_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Plot distributions of rewards for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(action):\n",
    "    '''\n",
    "    Returns the actual reward for the action taken.\n",
    "    The reward is sample from a normal distribution having mean defined in true_means and variance 1'''\n",
    "    return np.random.normal(loc=true_means[action], scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_agent(estimated_means):\n",
    "    max_estimate = np.max(estimated_means)\n",
    "    max_indexes = np.where(max_estimate == estimated_means)[0]\n",
    "    return np.random.choice(max_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mean(old_mean : float, reward : float, scaling_factor:float):\n",
    "    new_mean= old_mean + (reward - old_mean)*scaling_factor\n",
    "    return new_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_progress(reward_hist,  agents:list, epochs:int):\n",
    "    # TODO : Implement logic for plotting charts for various agents for comparison\n",
    "    plt.plot(np.transpose(reward_hist), labels=agents)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title('Reward vs epoch')\n",
    "\n",
    "    # TODO : Implement logic for optimal action plotting - how to regularize over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(agent, estimated_means, n_epochs:int):\n",
    "    # TODO : Implement a simulation of th k armed bandits problem\n",
    "    # TODO : Implement storage of the reward values, plot convergence\n",
    "    # TODO : Count percentage of time optimal action is taken as a function of time\n",
    "    for epoch in range(n_epochs):\n",
    "        action = agent(estimated_means)\n",
    "        estimated_means[action] = update_mean(estimated_means[action], reward = get_reward(action), epoch = epoch)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandit(estimated_values, epsilon:float, epochs:int):\n",
    "    action_hist = np.zeros(epochs, dtype=np.int8)\n",
    "    reward_hist = np.zeros(epochs, dtype=np.float16)\n",
    "    for epoch in range(epochs):\n",
    "        action_type = np.random.choice(a=np.ndarray(['explore', 'exploit']), p=[epsilon, (1-epsilon)])\n",
    "        greedy_choice = greedy_agent(estimated_means=estimated_values)\n",
    "        if 'exploit' == action_type:\n",
    "            action =greedy_choice\n",
    "        else:\n",
    "            action = np.random.choice(np.arange(len(estimated_values)))\n",
    "        \n",
    "        reward = get_reward(action)\n",
    "        action_hist[epoch] = action\n",
    "        reward_hist[epoch] = reward\n",
    "        estimated_values[action]=update_mean(estimated_values[action], reward, 1.0/(epoch+1))\n",
    "\n",
    "    return action_hist, reward_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'production (Python 3.8.16)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "action_history_greedy, reward_history_greedy = bandit(estimated_means, epsilon=0, epochs=500)\n",
    "action_history_ep_01, reward_history_ep_01 = bandit(estimated_means,epsilon=0.1, epochs=500)\n",
    "action_history_ep_001, reward_history_ep_001 = bandit(estimated_means, epsilon=0.01, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "production",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
